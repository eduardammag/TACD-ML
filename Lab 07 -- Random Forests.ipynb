{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b14463-2c4d-4a53-a8d1-def39d052cff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB 07 - Random Forest for Regression\n",
    "\n",
    "In this lab we will be extending the previous lab about Decision trees and build a Regression model using Random Forest.\n",
    "\n",
    "For simplicity, we will be using the same dataset as the previous lab (you can find it in ECLASS).\n",
    "\n",
    "**IMPORTANT:** For this lab, if you haven't finished your code from last week's lab on Decision trees, you will have the option to use the sklearn implementation for a regression tree. However, this doesn't mean that you should skip the previous lab. This is just so that you don't get behind with the content and you don't spend all your time today working on the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3323dc4-95cd-4564-8ccd-441019188fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29dfe0-10af-4871-ab66-e7cffe0685cc",
   "metadata": {},
   "source": [
    "As mentioned before, use the Boston Housing data and prepare your train/val/test split as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec95456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (404, 13), (404,)\n",
      "Validation set: (51, 13), (51,)\n",
      "Test set: (51, 13), (51,)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('bostonhousing.csv')\n",
    "\n",
    "# Step 2: Separate the target variable from the features\n",
    "X = data.drop(columns='medv')\n",
    "y = data['medv']\n",
    "\n",
    "# Step 3: Make an 80/10/10 train/validation/test split\n",
    "# First, split into 80% train and 20% temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now split the 20% temp into 50% validation and 50% test (i.e., 10% of the original data each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"Train set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6c793-4f97-49ff-a1f3-3e5ec2aa086b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1 -- Bootstrap\n",
    "\n",
    "Also known as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), this technique consists of making several samples with replacement of the original data, using each of the samples to train an estimator, and then aggregating the predictions using the average (this is also a type of model ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7aaba32-4c00-404c-9899-7a5759059598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, num_bags=10):\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags,\n",
    "    sample the dataset with replacement.\n",
    "    \n",
    "    This function does not return a copy\n",
    "    of the datapoints, but a list of indices\n",
    "    with compatible dimensionality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the \n",
    "        sampled datapoints in `X`\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bach will\n",
    "      match the number of datapoints in the given\n",
    "      dataset.\n",
    "    * The\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0)  # you can change the seed, or use 0 to replicate my results\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    bags = [rng.integers(low=0, high=n_samples, size=n_samples) for _ in range(num_bags)]\n",
    "    \n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff32d32b-f615-43da-b70e-aa8959d01093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85, 63, 51, 26, 30,  4,  7,  1, 17, 81, 64, 91, 50, 60, 97, 72, 63,\n",
       "       54, 55, 93, 27, 81, 67,  0, 39, 85, 55,  3, 76, 72, 84, 17,  8, 86,\n",
       "        2, 54,  8, 29, 48, 42, 40,  2,  0, 12,  0, 67, 52, 64, 25, 61, 76,\n",
       "       38, 46, 99, 80, 98, 37, 68, 95, 65, 84, 68, 70, 38, 87, 13, 57, 72,\n",
       "       84, 52, 37, 31, 42, 48, 71, 88,  7, 93, 53, 35, 67, 57, 25, 32, 71,\n",
       "       59, 50, 33, 76, 39, 32, 89, 26, 22, 71, 62,  4,  8, 37, 83],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_small = rng.random(size=(100,2))\n",
    "bags = bootstrap(X_small)\n",
    "bags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b5126-5aad-4c74-a7de-ad3935acd7e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2 -- Aggregation\n",
    "\n",
    "The second part of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baac85e0-2a36-4ee0-92a5-e31a01a70928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_regression(preds):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators.\n",
    "        All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions\n",
    "    \"\"\"\n",
    "    # Convert the list of ndarrays to a single 2D ndarray\n",
    "    preds_array = np.array(preds)\n",
    "    \n",
    "    # Compute the mean along the first axis (axis=0)\n",
    "    mean_preds = np.mean(preds_array, axis=0)\n",
    "    \n",
    "    return mean_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91f57a-6395-4285-ac01-e1099af84dde",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Random Forest for regression\n",
    "\n",
    "Using the functions you implemented above, it is now time to put all of them together to train several decision trees and then ensemble them to output a single prediction. For the random forest, however, we need to select a subset of features at each split on the decision tree. \n",
    "\n",
    "For this part, you can use the sklearn implementation of Random forest for regression as your estimator for each set of features and bags. See below an example of how to do this, and always remember to check the necessary documentation when using an external function.\n",
    "\n",
    "Some parameters you will have to set are: \n",
    "* num_features: number of features per estimator\n",
    "* min_samples: min number of samples per leaf node\n",
    "* max_depth: maximum depth of the decision tree (each estimator)\n",
    "* num_estimators: number of decision trees you will create using each bag and random set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71136bc0-c316-41d9-8832-c2db217101ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of sklearn Decision tree\n",
    "estimator = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "estimator.fit(X, y)\n",
    "estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48ecbd9d-9628-42d4-88ee-c3c3ea408e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.3092719587422976\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def bootstrap(X, num_bags=10):\n",
    "    rng = np.random.default_rng(0)  # you can change the seed, or use 0 to replicate my results\n",
    "    n_samples = X.shape[0]\n",
    "    bags = [rng.integers(low=0, high=n_samples, size=n_samples) for _ in range(num_bags)]\n",
    "    return bags\n",
    "\n",
    "def aggregate_regression(preds):\n",
    "    preds_array = np.array(preds)\n",
    "    mean_preds = np.mean(preds_array, axis=0)\n",
    "    return mean_preds\n",
    "\n",
    "def train_random_forest(X, y, num_features, min_samples, max_depth, num_estimators, num_bags):\n",
    "    # Generate bootstrap samples\n",
    "    bags = bootstrap(X, num_bags=num_bags)\n",
    "    \n",
    "    estimators = []\n",
    "\n",
    "    for bag_indices in bags:\n",
    "        # Select the bootstrap sample\n",
    "        X_bag = X[bag_indices]\n",
    "        y_bag = y[bag_indices]\n",
    "        \n",
    "        # Create and train the Decision Tree Regressor\n",
    "        model = DecisionTreeRegressor(\n",
    "            max_features=num_features,\n",
    "            min_samples_leaf=min_samples,\n",
    "            max_depth=max_depth,\n",
    "            random_state=0  # Ensuring reproducibility\n",
    "        )\n",
    "        model.fit(X_bag, y_bag)\n",
    "        estimators.append(model)\n",
    "    \n",
    "    return estimators\n",
    "\n",
    "def predict_random_forest(estimators, X):\n",
    "    # Collect predictions from all estimators\n",
    "    all_preds = [estimator.predict(X) for estimator in estimators]\n",
    "    # Aggregate the predictions\n",
    "    final_preds = aggregate_regression(all_preds)\n",
    "    return final_preds\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    cal_housing = fetch_california_housing()\n",
    "    X, y = cal_housing.data, cal_housing.target\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Parameters for the random forest\n",
    "    num_features = 5\n",
    "    min_samples = 2\n",
    "    max_depth = 10\n",
    "    num_estimators = 100\n",
    "    num_bags = 10\n",
    "\n",
    "    # Train the random forest\n",
    "    estimators = train_random_forest(X_train, y_train, num_features, min_samples, max_depth, num_estimators, num_bags)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = predict_random_forest(estimators, X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
